# 梯度累积功能使用说明

## 功能概述

本项目已添加梯度累积（Gradient Accumulation）功能，允许在显存有限的情况下模拟更大的batch size进行训练，同时保持学习率调度曲线不变。

## 主要修改

### 1. 新增参数

在 `utils/options.py` 中添加了新的命令行参数：

```bash
--gradient_accumulation_steps 2  # 梯度累积步数，默认为2
```

### 2. 训练逻辑修改

在 `processor/processor.py` 中修改了训练循环逻辑：

- **梯度清零时机**：只在累积开始时调用 `optimizer.zero_grad()`
- **损失缩放**：将损失除以累积步数，确保梯度大小正确
- **优化器步进**：只在累积步数达到时执行 `optimizer.step()`
- **学习率调度**：调整调度器步进频率，保持学习率曲线不变
- **剩余梯度处理**：在epoch结束时处理剩余的累积梯度

## 使用方法

### 基本用法

```bash
python train.py \
    --batch_size 12 \
    --gradient_accumulation_steps 2 \
    # 其他参数...
```

### 效果说明

- **实际有效batch size** = `batch_size` × `gradient_accumulation_steps`
- 例如：`batch_size=12`, `gradient_accumulation_steps=2` → 有效batch size = 24

### 示例脚本

参考 `script/train/gradient_accumulation_example.sh` 文件，其中包含完整的训练命令示例。

## 技术细节

### 1. 梯度累积原理

```python
# 传统训练
for batch in dataloader:
    optimizer.zero_grad()
    loss = model(batch)
    loss.backward()
    optimizer.step()

# 梯度累积训练
for i, batch in enumerate(dataloader):
    if i % accumulation_steps == 0:
        optimizer.zero_grad()
    
    loss = model(batch) / accumulation_steps  # 缩放损失
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
```

### 2. 学习率调度保持

为了保持学习率曲线不变，调度器的步进频率被调整为：

```python
if (n_iter + 1) % (scheduler_period * gradient_accumulation_steps) == 0:
    scheduler.step(scheduler_period)
```

### 3. 损失记录

损失值在记录时会乘以累积步数，确保显示的是原始损失值：

```python
meters['loss'].update(total_loss.item() * gradient_accumulation_steps, batch_size)
```

## 注意事项

1. **显存使用**：梯度累积会增加显存使用，因为需要保存中间梯度
2. **BatchNorm行为**：BatchNorm统计信息仍基于单个batch，可能与真实大batch略有不同
3. **学习率设置**：可以保持原有的学习率设置，无需调整
4. **调试建议**：建议先用小数据集测试，确保训练正常

## 兼容性

- 默认 `gradient_accumulation_steps=1`，与原有训练方式完全兼容
- 支持分布式训练
- 支持所有现有的损失函数和优化器
- 支持所有现有的学习率调度策略

## 性能对比

| 配置 | Batch Size | 累积步数 | 有效Batch Size | 显存使用 | 训练速度 |
|------|------------|----------|----------------|----------|----------|
| 原始 | 24 | 1 | 24 | 高 | 快 |
| 累积 | 12 | 2 | 24 | 中 | 稍慢 |
| 累积 | 8 | 3 | 24 | 低 | 慢 |

建议根据显存限制选择合适的配置。